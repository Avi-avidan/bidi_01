{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import locale\n",
    "import logging\n",
    "import signal\n",
    "import sys\n",
    "sys.path.append('/home/pi/snowboy/examples/Python3')\n",
    "\n",
    "from aiy.assistant.grpc import AssistantServiceClientWithLed\n",
    "from aiy.assistant.library import Assistant\n",
    "from aiy.board import Board\n",
    "from aiy.cloudspeech import CloudSpeechClient\n",
    "from aiy.assistant import auth_helpers\n",
    "import mod.snowboydecoder as snowboydecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import subprocess\n",
    "from aiy.voice import tts\n",
    "import aiy\n",
    "import RPi.GPIO as GPIO\n",
    "from gpiozero import PWMOutputDevice, Servo\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPIO.setmode(GPIO.BCM)\n",
    "# GPIO.setwarnings(False)\n",
    "# GPIO.setup(26, GPIO.OUT)\n",
    "\n",
    "# lidar_motor = Servo(6)#, min_pulse_width=2/1000, max_pulse_width=19/1000) #PWMOutputDevice(6) \n",
    "# lidar_motor.detach()\n",
    "\n",
    "intents_dict = {}\n",
    "intents_dict['turn_lights_on_hw'] = ['lights on', 'turn on the light', 'turn lights on']\n",
    "intents_dict['turn_lights_off_hw'] = ['lights off', 'lights out', 'turn off the light', 'turn lights off']\n",
    "intents_dict['say_ip_hw'] = [\"what's my ip\", 'ip address', 'ip', \"what's my ip address\"]\n",
    "intents_dict['positive_feedback'] = [\"that's great\", 'cool', 'thank you', \"that's nice\", 'wow']\n",
    "intents_dict['lidar_hw'] = ['look around', 'explore', 'lidar on', 'blink']\n",
    "intents_dict['exit'] = ['exit', 'shutdown', 'stop listening', 'enough', \"that's it for now\"]\n",
    "\n",
    "hints = [p for v in intents_dict.values() for p in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "positive_feedback_replies = ['no problem', 'no sweat', \"don't mention it\", 'you got it', 'your wish, my goalreply']\n",
    "\n",
    "def reply(text):\n",
    "    tts.say(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_scan_view():\n",
    "    querter = list(np.linspace(0,1,6))\n",
    "    half = querter + querter[::-1][1:]\n",
    "    return half + list(-1*np.array(half)[1:])\n",
    "\n",
    "def lidar_scan(time_to_blink=2.1, scan_view=[]):\n",
    "#     tts.say('lidar on')\n",
    "    if len(scan_view) == 0:\n",
    "        scan_view = get_scan_view()\n",
    "    \n",
    "    for i, val in enumerate(scan_view):\n",
    "        GPIO.output(26, i%2 == 0)\n",
    "        lidar_motor.value = val\n",
    "        time.sleep(time_to_blink/len(scan_view))\n",
    "    lidar_motor.detach()\n",
    "    GPIO.output(26, False)\n",
    "    \n",
    "# lidar_scan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfmini import TFmini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pigpiod -p 8889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PIGPIO_ADDR=0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PIGPIO_PORT=8889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pigpio\n",
    "pi = pigpio.pi(host='0.0.0.0', port=8889)\n",
    "RX = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi.set_mode(RX, pigpio.INPUT)\n",
    "pi.bb_serial_read_open(RX, 115200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pigpio\n",
    "import time\n",
    "\n",
    "def getTFminiData():\n",
    "  while True:\n",
    "    #print(\"#############\")\n",
    "    time.sleep(0.05)\t#change the value if needed\n",
    "    (count, recv) = pi.bb_serial_read(RX)\n",
    "#     print(count, recv)\n",
    "    if count > 8:\n",
    "      for i in range(0, count-9):\n",
    "        if recv[i] == 89 and recv[i+1] == 89: # 0x59 is 89\n",
    "          checksum = 0\n",
    "          for j in range(0, 8):\n",
    "            checksum += recv[i+j]\n",
    "          checksum = checksum % 256\n",
    "          if checksum == recv[i+8]:\n",
    "            distance = recv[i+2] + recv[i+3] * 256\n",
    "            strength = recv[i+4] + recv[i+5] * 256\n",
    "            if distance <= 1200 and strength < 2000:\n",
    "              print(distance, strength) \n",
    "            #else:\n",
    "              # raise ValueError('distance error: %d' % distance)\t\n",
    "            #i = i + 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lidar_read():\n",
    "    return pi.bb_serial_read(RX)\n",
    "\n",
    "def get_lidar(num_reads=3):\n",
    "    lid_out = []\n",
    "    while(len(lid_out) < num_reads):\n",
    "        count, recv = lidar_read()\n",
    "        if count > 8:\n",
    "            for i in range(0, count-9):\n",
    "                if recv[i] == 89 and recv[i+1] == 89: # 0x59 is 89\n",
    "                    checksum = 0\n",
    "                    for j in range(0, 8):\n",
    "                        checksum += recv[i+j]\n",
    "                    checksum = checksum % 256\n",
    "                    if checksum == recv[i+8]:\n",
    "                        distance = recv[i+2] + recv[i+3] * 256\n",
    "                        strength = recv[i+4] + recv[i+5] * 256\n",
    "                        if distance <= 1200 and strength < 2000:\n",
    "                            lid_out.append([distance, strength])\n",
    "                            if len(lid_out) >= num_reads:\n",
    "                                return np.array(lid_out)\n",
    "                time.sleep(0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lidar_read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     try:\n",
    "#         getTFminiData()\n",
    "#     except:  \n",
    "#         pi.bb_serial_read_close(RX)\n",
    "#         pi.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import google.assistant.embedded.v1alpha1.embedded_assistant_pb2_grpc as assist_grpc\n",
    "\n",
    "channel = grpc.insecure_channel('localhost:9000')\n",
    "assistant = assist_grpc.EmbeddedAssistantStub(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG, _ = locale.getdefaultlocale()\n",
    "HOTWORDS = ['/home/pi/Downloads/hi_bidi.pmdl',\n",
    "            '/home/pi/Downloads/bidi.pmdl']\n",
    "# credentials = auth_helpers.get_assistant_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_hints(language_code):\n",
    "    if language_code.startswith('en_'):\n",
    "        return hints\n",
    "    return None\n",
    "\n",
    "def power_off_pi():\n",
    "    tts.say('Good bye!')\n",
    "    subprocess.call('sudo shutdown now', shell=True)\n",
    "\n",
    "def lights_on():\n",
    "    tts.say('lights on baby')\n",
    "    GPIO.output(26, True)\n",
    "    \n",
    "def blink(gpio_num=26, time_to_blink=3):\n",
    "    for i in range(int(time_to_blink/0.5)):\n",
    "        GPIO.output(gpio_num, True)\n",
    "        time.sleep(0.25)\n",
    "        GPIO.output(gpio_num, False)\n",
    "        time.sleep(0.25)\n",
    "\n",
    "def lidar_scan():\n",
    "    tts.say('lidar on')\n",
    "    blink(time_to_blink=1.8)\n",
    "    lidar_motor.min()\n",
    "    time.sleep(time_to_blink/3)\n",
    "#     lidar_motor.value = 0.75\n",
    "    lidar_motor.mid()\n",
    "    time.sleep(time_to_blink/3)\n",
    "#     lidar_motor.value = 0.25\n",
    "    lidar_motor.max()\n",
    "    time.sleep(time_to_blink/3)\n",
    "#     lidar_motor.value = 0\n",
    "    lidar_motor.mid()\n",
    "    \n",
    "def lights_off():\n",
    "    tts.say('lights off')\n",
    "    GPIO.output(26, False)\n",
    "\n",
    "def reboot_pi():\n",
    "    tts.say('See you in a bit!')\n",
    "    subprocess.call('sudo reboot', shell=True)\n",
    "\n",
    "\n",
    "def say_ip():\n",
    "    ip_address = subprocess.check_output(\"hostname -I | cut -d' ' -f1\", shell=True)\n",
    "    tts.say('My IP address is %s' % ip_address.decode('utf-8'))\n",
    "\n",
    "\n",
    "def process_text(text, led, event):\n",
    "    logging.info(event)\n",
    "    \n",
    "    if event.type == EventType.ON_START_FINISHED:\n",
    "        led.state = Led.BEACON_DARK  # Ready.\n",
    "        print('Say \"OK, Google\" then speak, or press Ctrl+C to quit...')\n",
    "        \n",
    "    elif event.type == EventType.ON_CONVERSATION_TURN_STARTED:\n",
    "        led.state = Led.ON  # Listening.\n",
    "        \n",
    "    elif event.type == EventType.ON_RECOGNIZING_SPEECH_FINISHED and event.args:\n",
    "        print('You said:', event.args['text'])\n",
    "        text = event.args['text'].lower()\n",
    "        if text == 'power off':\n",
    "            assistant.stop_conversation()\n",
    "            power_off_pi()\n",
    "        elif text == 'reboot':\n",
    "            assistant.stop_conversation()\n",
    "            reboot_pi()\n",
    "        elif text == 'ip address':\n",
    "            assistant.stop_conversation()\n",
    "            say_ip()\n",
    "\n",
    "def process_event(assistant, led, event):\n",
    "    logging.info(event)\n",
    "    \n",
    "    if event.type == EventType.ON_START_FINISHED:\n",
    "        led.state = Led.BEACON_DARK  # Ready.\n",
    "        print('Say \"OK, Google\" then speak, or press Ctrl+C to quit...')\n",
    "        \n",
    "    elif event.type == EventType.ON_CONVERSATION_TURN_STARTED:\n",
    "        led.state = Led.ON  # Listening.\n",
    "        \n",
    "    elif event.type == EventType.ON_RECOGNIZING_SPEECH_FINISHED and event.args:\n",
    "        print('You said:', event.args['text'])\n",
    "        text = event.args['text'].lower()\n",
    "        if text == 'power off':\n",
    "            assistant.stop_conversation()\n",
    "            power_off_pi()\n",
    "        elif text == 'reboot':\n",
    "            assistant.stop_conversation()\n",
    "            reboot_pi()\n",
    "        elif text == 'ip address':\n",
    "            assistant.stop_conversation()\n",
    "            say_ip()\n",
    "            \n",
    "    elif event.type == EventType.ON_END_OF_UTTERANCE:\n",
    "        led.state = Led.PULSE_QUICK  # Thinking.\n",
    "        \n",
    "    elif (event.type == EventType.ON_CONVERSATION_TURN_FINISHED\n",
    "          or event.type == EventType.ON_CONVERSATION_TURN_TIMEOUT\n",
    "          or event.type == EventType.ON_NO_RESPONSE):\n",
    "        led.state = Led.BEACON_DARK  # Ready.\n",
    "        \n",
    "    elif event.type == EventType.ON_ASSISTANT_ERROR and event.args and event.args['is_fatal']:\n",
    "        sys.exit(1)            \n",
    "\n",
    "def text_nevigator(text):\n",
    "    text = text.lower()\n",
    "    print('bidi heard:', text)\n",
    "    if text in intents_dict['turn_lights_on_hw']:\n",
    "        lights_on()\n",
    "    elif text in intents_dict['turn_lights_off_hw']:\n",
    "        lights_off()\n",
    "    elif text in intents_dict['say_ip_hw']:\n",
    "        say_ip()\n",
    "    elif text in intents_dict['positive_feedback']:\n",
    "        reply(np.random.choice(positive_feedback_replies))\n",
    "    elif text in intents_dict['lidar_hw']:\n",
    "        lidar_scan()\n",
    "    elif text in intents_dict['exit']:\n",
    "        reply('bye bye')\n",
    "        return 'exit now'\n",
    "    else:\n",
    "        print()\n",
    "    return text\n",
    "        \n",
    "def main():\n",
    "    tts.say('bidi loading')\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    signal.signal(signal.SIGTERM, lambda signum, frame: sys.exit(0))\n",
    "\n",
    "    detector = snowboydecoder.HotwordDetector(HOTWORDS, sensitivity=0.5)\n",
    "\n",
    "    client = CloudSpeechClient()\n",
    "#     with Board() as board, Assistant(credentials) as assistant:\n",
    "#         grpc_assistant = AssistantServiceClientWithLed(board=board,\n",
    "#                                                   volume_percentage=60,\n",
    "#                                                   language_code=LANG)\n",
    "#         assistant = Assistant(credentials) \n",
    "    \n",
    "    listen = 1\n",
    "    while(listen):\n",
    "        logging.info('say hi bidi to start conversation ...')\n",
    "        detector.start()\n",
    "        logging.info('talk to bidi, dont be shy...')\n",
    "#             assistant.Converse()\n",
    "#             try:\n",
    "#                 for event in assistant.Converse():\n",
    "#                     process_event(assistant, board.led, event)\n",
    "#             except Exception as e: \n",
    "#                 print(e)\n",
    "#                 break\n",
    "#             assistant.start_conversation()\n",
    "        text = client.recognize(language_code=LANG,\n",
    "                                hint_phrases=get_hints(LANG))\n",
    "        \n",
    "        if text:\n",
    "            print(text)\n",
    "            text = text_nevigator(text)\n",
    "            if text == 'exit now':\n",
    "                print('breaking listening loop. bye bye')\n",
    "                listen = 0\n",
    "            \n",
    "\n",
    "#             print(assistant.send_text_query(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:say hi bidi to start conversation ...\n",
      "DEBUG:snowboy:detecting...\n",
      "INFO:snowboy:detect started.\n",
      "INFO:snowboy:detect stopped.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a1ab73aeb3c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# if __name__ == '__main__':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-99297cd297de>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlisten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'say hi bidi to start conversation ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'talk to bidi, dont be shy...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;31m#             assistant.Converse()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIY-projects-python/src/mod/snowboydecoder.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, detected_callback, interrupt_check, sleep_time)\u001b[0m\n\u001b[1;32m    136\u001b[0m                                          \u001b[0mchunk_duration_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                                          \u001b[0mon_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_detect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                                          on_stop=self.stop_detect):\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRunDetection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIY-projects-python/src/aiy/voice/audio.py\u001b[0m in \u001b[0;36mrecord\u001b[0;34m(self, fmt, chunk_duration_sec, device, num_chunks, on_start, on_stop, filename)\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = auth_helpers.get_assistant_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "# import google.oauth2.credentials\n",
    "import json\n",
    "\n",
    "# with io.open('/home/pi/assistant.json', 'r') as f:\n",
    "#     credentials = google.oauth2.credentials.Credentials(token=None,\n",
    "#                                                         **json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "channel = grpc.insecure_channel('localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import google.assistant.embedded.v1alpha1.embedded_assistant_pb2_grpc as assist_grpc\n",
    "\n",
    "channel = grpc.insecure_channel('localhost:9000')\n",
    "assistant = assist_grpc.EmbeddedAssistantStub(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 1 required positional argument: 'request_iterator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-cc519c2e2e4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0massistant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __call__() missing 1 required positional argument: 'request_iterator'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.assistant.embedded.v1alpha1.embedded_assistant_pb2 as assist\n",
    "\n",
    "def generate_assist_requests():\n",
    "    yield assist.AssistConfig(\n",
    "        audio_in_config=assist.AudioInConfig(\n",
    "            encoding='LINEAR16',\n",
    "            sample_rate_hertz=16000,\n",
    "        ),\n",
    "        audio_out_config=assist.AudioOutConfig(\n",
    "            encoding='LINEAR16',\n",
    "            sample_rate_hertz=16000,\n",
    "        ),\n",
    "        device_config=assist.DeviceConfig(\n",
    "            device_id=device_id,\n",
    "            device_model_id=device_model_id,\n",
    "        )\n",
    "    )\n",
    "    for data in acquire_audio_data():\n",
    "        yield assist.AssistRequest(audio_in=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assist_response_generator = generate_assist_requests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'google.assistant.embedded.v1alpha1.embedded_assistant_pb2' has no attribute 'AssistConfig'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-940539121a38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0massist_response\u001b[0m \u001b[0;32min\u001b[0m \u001b[0massist_response_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEND_OF_UTTERANCE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m        \u001b[0mstop_acquiring_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeech_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscript\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeech_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-e048e7f2c245>\u001b[0m in \u001b[0;36mgenerate_assist_requests\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_assist_requests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     yield assist.AssistConfig(\n\u001b[0m\u001b[1;32m      5\u001b[0m         audio_in_config=assist.AudioInConfig(\n\u001b[1;32m      6\u001b[0m             \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LINEAR16'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'google.assistant.embedded.v1alpha1.embedded_assistant_pb2' has no attribute 'AssistConfig'"
     ]
    }
   ],
   "source": [
    "for assist_response in assist_response_generator:\n",
    "    if resp.event_type == END_OF_UTTERANCE:\n",
    "       stop_acquiring_audio()\n",
    "    if resp.speech_results:\n",
    "        print(' '.join(r.transcript for r in resp.speech_results))\n",
    "    if resp.dialog_state_out.supplemental_display_text:\n",
    "       print(resp.dialog_state_out.supplemental_display_text)\n",
    "    if len(resp.audio_out.audio_data) > 0:\n",
    "       playback_audio_data(resp.audio_out.audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:google.auth.transport.requests:Making request: POST https://oauth2.googleapis.com/token\n",
      "DEBUG:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): oauth2.googleapis.com\n",
      "DEBUG:requests.packages.urllib3.connectionpool:https://oauth2.googleapis.com:443 \"POST /token HTTP/1.1\" 200 None\n"
     ]
    }
   ],
   "source": [
    "verbose = True\n",
    "credentials = auth_helpers.get_assistant_credentials()\n",
    "http_request = google.auth.transport.requests.Request()\n",
    "api_endpoint = 'embeddedassistant.googleapis.com'\n",
    "#         credentials.refresh(http_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Connecting to embeddedassistant.googleapis.com\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO)\n",
    "\n",
    "grpc_channel = google.auth.transport.grpc.secure_authorized_channel(\n",
    "    credentials, http_request, api_endpoint)\n",
    "logging.info('Connecting to %s', api_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import platform\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from google.assistant.library.event import EventType\n",
    "\n",
    "from aiy.assistant import auth_helpers\n",
    "from aiy.assistant.library import Assistant\n",
    "from aiy.board import Board, Led\n",
    "from aiy.voice import tts\n",
    "\n",
    "def power_off_pi():\n",
    "    tts.say('Good bye!')\n",
    "    subprocess.call('sudo shutdown now', shell=True)\n",
    "\n",
    "\n",
    "def reboot_pi():\n",
    "    tts.say('See you in a bit!')\n",
    "    subprocess.call('sudo reboot', shell=True)\n",
    "\n",
    "\n",
    "def say_ip():\n",
    "    ip_address = subprocess.check_output(\"hostname -I | cut -d' ' -f1\", shell=True)\n",
    "    tts.say('My IP address is %s' % ip_address.decode('utf-8'))\n",
    "\n",
    "\n",
    "def process_event(assistant, led, event):\n",
    "    logging.info(event)\n",
    "    \n",
    "    if event.type == EventType.ON_START_FINISHED:\n",
    "        led.state = Led.BEACON_DARK  # Ready.\n",
    "        print('Say \"OK, Google\" then speak, or press Ctrl+C to quit...')\n",
    "        \n",
    "    elif event.type == EventType.ON_CONVERSATION_TURN_STARTED:\n",
    "        led.state = Led.ON  # Listening.\n",
    "        \n",
    "    elif event.type == EventType.ON_RECOGNIZING_SPEECH_FINISHED and event.args:\n",
    "        print('You said:', event.args['text'])\n",
    "        text = event.args['text'].lower()\n",
    "        if text == 'power off':\n",
    "            assistant.stop_conversation()\n",
    "            power_off_pi()\n",
    "        elif text == 'reboot':\n",
    "            assistant.stop_conversation()\n",
    "            reboot_pi()\n",
    "        elif text == 'ip address':\n",
    "            assistant.stop_conversation()\n",
    "            say_ip()\n",
    "            \n",
    "    elif event.type == EventType.ON_END_OF_UTTERANCE:\n",
    "        led.state = Led.PULSE_QUICK  # Thinking.\n",
    "        \n",
    "    elif (event.type == EventType.ON_CONVERSATION_TURN_FINISHED\n",
    "          or event.type == EventType.ON_CONVERSATION_TURN_TIMEOUT\n",
    "          or event.type == EventType.ON_NO_RESPONSE):\n",
    "        led.state = Led.BEACON_DARK  # Ready.\n",
    "        \n",
    "    elif event.type == EventType.ON_ASSISTANT_ERROR and event.args and event.args['is_fatal']:\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    credentials = auth_helpers.get_assistant_credentials()\n",
    "    with Board() as board, Assistant(credentials) as assistant:\n",
    "        for event in assistant.start():\n",
    "            process_event(assistant, board.led, event)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/pi/AIY-projects-python/src/assistant-sdk-python/google-assistant-sdk/googlesamples/assistant/grpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sounddevice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b9350a793940>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0massistant_helpers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0maudio_helpers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbrowser_helpers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_helpers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AIY-projects-python/src/assistant-sdk-python/google-assistant-sdk/googlesamples/assistant/grpc/audio_helpers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msounddevice\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sounddevice'"
     ]
    }
   ],
   "source": [
    "# imports from the original Google pushtotalk.py file\n",
    "import concurrent.futures\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import os.path\n",
    "import pathlib2 as pathlib\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "import click\n",
    "import grpc\n",
    "import google.auth.transport.grpc\n",
    "import google.auth.transport.requests\n",
    "import google.oauth2.credentials\n",
    "\n",
    "from google.assistant.embedded.v1alpha2 import (\n",
    "    embedded_assistant_pb2,\n",
    "    embedded_assistant_pb2_grpc\n",
    ")\n",
    "# from tenacity import retry, stop_after_attempt, retry_if_exception\n",
    "\n",
    "import assistant_helpers\n",
    "import audio_helpers\n",
    "import browser_helpers\n",
    "import device_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'voice_engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-53a445e63a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# my imports for the mechanical functions, motor drivers, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# from Raspi_PWM_Servo_Driver import PWM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvoice_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvoice_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannel_picker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChannelPicker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvoice_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkws\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKWS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'voice_engine'"
     ]
    }
   ],
   "source": [
    "# my imports for the mechanical functions, motor drivers, etc.\n",
    "from Raspi_PWM_Servo_Driver import PWM\n",
    "from voice_engine.source import Source\n",
    "from voice_engine.channel_picker import ChannelPicker\n",
    "from voice_engine.kws import KWS\n",
    "from voice_engine.doa_respeaker_4mic_array import DOA\n",
    "from pixels import pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original pushtotalk.py file Copyright (C) 2017 Google Inc.\n",
    "# modified by @captstephan for T3 project\n",
    "#\n",
    "\n",
    "\n",
    "# set up Google Assistant variables (from pushtotalk.py)\n",
    "ASSISTANT_API_ENDPOINT = 'embeddedassistant.googleapis.com'\n",
    "END_OF_UTTERANCE = embedded_assistant_pb2.AssistResponse.END_OF_UTTERANCE\n",
    "DIALOG_FOLLOW_ON = embedded_assistant_pb2.DialogStateOut.DIALOG_FOLLOW_ON\n",
    "CLOSE_MICROPHONE = embedded_assistant_pb2.DialogStateOut.CLOSE_MICROPHONE\n",
    "PLAYING = embedded_assistant_pb2.ScreenOutConfig.PLAYING\n",
    "DEFAULT_GRPC_DEADLINE = 60 * 3 + 5\n",
    "\n",
    "# set up items for motor hats\n",
    "# Initialise the PWM device using the default address\n",
    "pwm = PWM(0x6F)\n",
    "\n",
    "# set max and min, servo0=Horiz, servo1=vert\n",
    "servoMin0 = 155  # Min pulse length out of 4096\n",
    "servoMid0 = 370\n",
    "servoMax0 = 585  # Max pulse length out of 4096\n",
    "\n",
    "servoMin1 = 410  # Min pulse length out of 4096\n",
    "servoMid1 = 530\n",
    "servoMax1 = 650  # Max pulse length out of 4096\n",
    "\n",
    "pwm.setPWMFreq(60)  # Set frequency to 60 Hz\n",
    "\n",
    "\n",
    "# class assignment from pushtotalk.py file:\n",
    "class SampleAssistant(object):\n",
    "    \"\"\"Sample Assistant that supports conversations and device actions.\n",
    "\n",
    "    Args:\n",
    "      device_model_id: identifier of the device model.\n",
    "      device_id: identifier of the registered device instance.\n",
    "      conversation_stream(ConversationStream): audio stream\n",
    "        for recording query and playing back assistant answer.\n",
    "      channel: authorized gRPC channel for connection to the\n",
    "        Google Assistant API.\n",
    "      deadline_sec: gRPC deadline in seconds for Google Assistant API call.\n",
    "      device_handler: callback for device actions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, language_code, device_model_id, device_id,\n",
    "                 conversation_stream, display,\n",
    "                 channel, deadline_sec, device_handler):\n",
    "        self.language_code = language_code\n",
    "        self.device_model_id = device_model_id\n",
    "        self.device_id = device_id\n",
    "        self.conversation_stream = conversation_stream\n",
    "        self.display = display\n",
    "\n",
    "        # Opaque blob provided in AssistResponse that,\n",
    "        # when provided in a follow-up AssistRequest,\n",
    "        # gives the Assistant a context marker within the current state\n",
    "        # of the multi-Assist()-RPC \"conversation\".\n",
    "        # This value, along with MicrophoneMode, supports a more natural\n",
    "        # \"conversation\" with the Assistant.\n",
    "        self.conversation_state = None\n",
    "        # Force reset of first conversation.\n",
    "        self.is_new_conversation = True\n",
    "\n",
    "        # Create Google Assistant API gRPC client.\n",
    "        self.assistant = embedded_assistant_pb2_grpc.EmbeddedAssistantStub(\n",
    "            channel\n",
    "        )\n",
    "        self.deadline = deadline_sec\n",
    "\n",
    "        self.device_handler = device_handler\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, etype, e, traceback):\n",
    "        if e:\n",
    "            return False\n",
    "        self.conversation_stream.close()\n",
    "\n",
    "    def is_grpc_error_unavailable(e):\n",
    "        is_grpc_error = isinstance(e, grpc.RpcError)\n",
    "        if is_grpc_error and (e.code() == grpc.StatusCode.UNAVAILABLE):\n",
    "            logging.error('grpc unavailable error: %s', e)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    @retry(reraise=True, stop=stop_after_attempt(3),\n",
    "           retry=retry_if_exception(is_grpc_error_unavailable))\n",
    "    def assist(self):\n",
    "        \"\"\"Send a voice request to the Assistant and playback the response.\n",
    "\n",
    "        Returns: True if conversation should continue.\n",
    "        \"\"\"\n",
    "        continue_conversation = False\n",
    "        device_actions_futures = []\n",
    "\n",
    "        self.conversation_stream.start_recording()\n",
    "        logging.info('Recording audio request.')\n",
    "\n",
    "        def iter_log_assist_requests():\n",
    "            for c in self.gen_assist_requests():\n",
    "                assistant_helpers.log_assist_request_without_audio(c)\n",
    "                yield c\n",
    "            logging.debug('Reached end of AssistRequest iteration.')\n",
    "\n",
    "        # This generator yields AssistResponse proto messages\n",
    "        # received from the gRPC Google Assistant API.\n",
    "        for resp in self.assistant.Assist(iter_log_assist_requests(),\n",
    "                                          self.deadline):\n",
    "            assistant_helpers.log_assist_response_without_audio(resp)\n",
    "            if resp.event_type == END_OF_UTTERANCE:\n",
    "                logging.info('End of audio request detected.')\n",
    "                logging.info('Stopping recording.')\n",
    "                self.conversation_stream.stop_recording()\n",
    "            if resp.speech_results:\n",
    "                logging.info('Transcript of user request: \"%s\".',\n",
    "                             ' '.join(r.transcript\n",
    "                                      for r in resp.speech_results))\n",
    "            if len(resp.audio_out.audio_data) > 0:\n",
    "                if not self.conversation_stream.playing:\n",
    "                    self.conversation_stream.stop_recording()\n",
    "                    self.conversation_stream.start_playback()\n",
    "                    logging.info('Playing assistant response.')\n",
    "                self.conversation_stream.write(resp.audio_out.audio_data)\n",
    "            if resp.dialog_state_out.conversation_state:\n",
    "                conversation_state = resp.dialog_state_out.conversation_state\n",
    "                logging.debug('Updating conversation state.')\n",
    "                self.conversation_state = conversation_state\n",
    "            if resp.dialog_state_out.volume_percentage != 0:\n",
    "                volume_percentage = resp.dialog_state_out.volume_percentage\n",
    "                logging.info('Setting volume to %s%%', volume_percentage)\n",
    "                self.conversation_stream.volume_percentage = volume_percentage\n",
    "            if resp.dialog_state_out.microphone_mode == DIALOG_FOLLOW_ON:\n",
    "                continue_conversation = True\n",
    "                logging.info('Expecting follow-on query from user.')\n",
    "            elif resp.dialog_state_out.microphone_mode == CLOSE_MICROPHONE:\n",
    "                continue_conversation = False\n",
    "            if resp.device_action.device_request_json:\n",
    "                device_request = json.loads(\n",
    "                    resp.device_action.device_request_json\n",
    "                )\n",
    "                fs = self.device_handler(device_request)\n",
    "                if fs:\n",
    "                    device_actions_futures.extend(fs)\n",
    "            if self.display and resp.screen_out.data:\n",
    "                system_browser = browser_helpers.system_browser\n",
    "                system_browser.display(resp.screen_out.data)\n",
    "\n",
    "        if len(device_actions_futures):\n",
    "            logging.info('Waiting for device executions to complete.')\n",
    "            concurrent.futures.wait(device_actions_futures)\n",
    "\n",
    "        logging.info('Finished playing assistant response.')\n",
    "        self.conversation_stream.stop_playback()\n",
    "        return continue_conversation\n",
    "\n",
    "    def gen_assist_requests(self):\n",
    "        \"\"\"Yields: AssistRequest messages to send to the API.\"\"\"\n",
    "\n",
    "        config = embedded_assistant_pb2.AssistConfig(\n",
    "            audio_in_config=embedded_assistant_pb2.AudioInConfig(\n",
    "                encoding='LINEAR16',\n",
    "                sample_rate_hertz=self.conversation_stream.sample_rate,\n",
    "            ),\n",
    "            audio_out_config=embedded_assistant_pb2.AudioOutConfig(\n",
    "                encoding='LINEAR16',\n",
    "                sample_rate_hertz=self.conversation_stream.sample_rate,\n",
    "                volume_percentage=self.conversation_stream.volume_percentage,\n",
    "            ),\n",
    "            dialog_state_in=embedded_assistant_pb2.DialogStateIn(\n",
    "                language_code=self.language_code,\n",
    "                conversation_state=self.conversation_state,\n",
    "                is_new_conversation=self.is_new_conversation,\n",
    "            ),\n",
    "            device_config=embedded_assistant_pb2.DeviceConfig(\n",
    "                device_id=self.device_id,\n",
    "                device_model_id=self.device_model_id,\n",
    "            )\n",
    "        )\n",
    "        if self.display:\n",
    "            config.screen_out_config.screen_mode = PLAYING\n",
    "        # Continue current conversation with later requests.\n",
    "        self.is_new_conversation = False\n",
    "        # The first AssistRequest must contain the AssistConfig\n",
    "        # and no audio data.\n",
    "        yield embedded_assistant_pb2.AssistRequest(config=config)\n",
    "        for data in self.conversation_stream:\n",
    "            # Subsequent requests need audio data, but not config.\n",
    "            yield embedded_assistant_pb2.AssistRequest(audio_in=data)\n",
    "\n",
    "\n",
    "@click.command()\n",
    "@click.option('--api-endpoint', default=ASSISTANT_API_ENDPOINT,\n",
    "              metavar='<api endpoint>', show_default=True,\n",
    "              help='Address of Google Assistant API service.')\n",
    "@click.option('--credentials',\n",
    "              metavar='<credentials>', show_default=True,\n",
    "              default=os.path.join(click.get_app_dir('google-oauthlib-tool'),\n",
    "                                   'credentials.json'),\n",
    "              help='Path to read OAuth2 credentials.')\n",
    "@click.option('--project-id',\n",
    "              metavar='<project id>',\n",
    "              help=('Google Developer Project ID used for registration '\n",
    "                    'if --device-id is not specified'))\n",
    "@click.option('--device-model-id',\n",
    "              metavar='<device model id>',\n",
    "              help=(('Unique device model identifier, '\n",
    "                     'if not specifed, it is read from --device-config')))\n",
    "@click.option('--device-id',\n",
    "              metavar='<device id>',\n",
    "              help=(('Unique registered device instance identifier, '\n",
    "                     'if not specified, it is read from --device-config, '\n",
    "                     'if no device_config found: a new device is registered '\n",
    "                     'using a unique id and a new device config is saved')))\n",
    "@click.option('--device-config', show_default=True,\n",
    "              metavar='<device config>',\n",
    "              default=os.path.join(\n",
    "                  click.get_app_dir('googlesamples-assistant'),\n",
    "                  'device_config.json'),\n",
    "              help='Path to save and restore the device configuration')\n",
    "@click.option('--lang', show_default=True,\n",
    "              metavar='<language code>',\n",
    "              default='en-US',\n",
    "              help='Language code of the Assistant')\n",
    "@click.option('--display', is_flag=True, default=False,\n",
    "              help='Enable visual display of Assistant responses in HTML.')\n",
    "@click.option('--verbose', '-v', is_flag=True, default=False,\n",
    "              help='Verbose logging.')\n",
    "@click.option('--input-audio-file', '-i',\n",
    "              metavar='<input file>',\n",
    "              help='Path to input audio file. '\n",
    "              'If missing, uses audio capture')\n",
    "@click.option('--output-audio-file', '-o',\n",
    "              metavar='<output file>',\n",
    "              help='Path to output audio file. '\n",
    "              'If missing, uses audio playback')\n",
    "@click.option('--audio-sample-rate',\n",
    "              default=audio_helpers.DEFAULT_AUDIO_SAMPLE_RATE,\n",
    "              metavar='<audio sample rate>', show_default=True,\n",
    "              help='Audio sample rate in hertz.')\n",
    "@click.option('--audio-sample-width',\n",
    "              default=audio_helpers.DEFAULT_AUDIO_SAMPLE_WIDTH,\n",
    "              metavar='<audio sample width>', show_default=True,\n",
    "              help='Audio sample width in bytes.')\n",
    "@click.option('--audio-iter-size',\n",
    "              default=audio_helpers.DEFAULT_AUDIO_ITER_SIZE,\n",
    "              metavar='<audio iter size>', show_default=True,\n",
    "              help='Size of each read during audio stream iteration in bytes.')\n",
    "@click.option('--audio-block-size',\n",
    "              default=audio_helpers.DEFAULT_AUDIO_DEVICE_BLOCK_SIZE,\n",
    "              metavar='<audio block size>', show_default=True,\n",
    "              help=('Block size in bytes for each audio device '\n",
    "                    'read and write operation.'))\n",
    "@click.option('--audio-flush-size',\n",
    "              default=audio_helpers.DEFAULT_AUDIO_DEVICE_FLUSH_SIZE,\n",
    "              metavar='<audio flush size>', show_default=True,\n",
    "              help=('Size of silence data in bytes written '\n",
    "                    'during flush operation'))\n",
    "@click.option('--grpc-deadline', default=DEFAULT_GRPC_DEADLINE,\n",
    "              metavar='<grpc deadline>', show_default=True,\n",
    "              help='gRPC deadline in seconds')\n",
    "@click.option('--once', default=False, is_flag=True,\n",
    "              help='Force termination after a single conversation.')\n",
    "\n",
    "def main(api_endpoint, credentials, project_id,\n",
    "         device_model_id, device_id, device_config,\n",
    "         lang, display, verbose,\n",
    "         input_audio_file, output_audio_file,\n",
    "         audio_sample_rate, audio_sample_width,\n",
    "         audio_iter_size, audio_block_size, audio_flush_size,\n",
    "         grpc_deadline, once, *args, **kwargs):\n",
    "\n",
    "# Inserted the following code to set up the snowboy keyword activation using \"Hey T3\"\n",
    "    src = Source(rate=16000, channels=4, frames_size=320)\n",
    "    ch1 = ChannelPicker(channels=4, pick=1)\n",
    "    kws = KWS()\n",
    "    doa = DOA(rate=16000)\n",
    "\n",
    "    src.link(ch1)\n",
    "    ch1.link(kws)\n",
    "    src.link(doa)\n",
    "    pixels.listen()\n",
    "    pwm.setPWM(0, 0, 370)\n",
    "    pwm.setPWM(1, 0, 640)\n",
    "\n",
    "# When snowboy detects the custom keyword, set the camera position to near direction of voice\n",
    "    def on_detected(keyword):\n",
    "        position = doa.get_direction()\n",
    "        pixels.wakeup(position)\n",
    "        print('detected {} at direction {}'.format(keyword, position))\n",
    "        if position >= 30 and position <= 180:\n",
    "            pwm.setPWM(0, 0, 175)\n",
    "            pwm.setPWM(1, 0, 500)\n",
    "        elif position > 180 and position <= 330:\n",
    "            pwm.setPWM(0, 0, 560)\n",
    "            pwm.setPWM(1, 0, 500)\n",
    "        elif position > 330 or position < 30:\n",
    "            pwm.setPWM(0, 0, 370)\n",
    "            pwm.setPWM(1, 0, 6200)\n",
    "        else:\n",
    "            pwm.setPWM(0, 0, 370)\n",
    "            pwm.setPWM(1, 0, 640)\n",
    "\n",
    "    # end of stuff I inserted\n",
    "\n",
    "# Setup logging.\n",
    "assistant_helpers\n",
    "\n",
    "    # Configure audio source and sink.\n",
    "    audio_device = None\n",
    "    if input_audio_file:\n",
    "        audio_source = audio_helpers.WaveSource(\n",
    "            open(input_audio_file, 'rb'),\n",
    "            sample_rate=audio_sample_rate,\n",
    "            sample_width=audio_sample_width\n",
    "        )\n",
    "    else:\n",
    "        audio_source = audio_device = (\n",
    "            audio_device or audio_helpers.SoundDeviceStream(\n",
    "                sample_rate=audio_sample_rate,\n",
    "                sample_width=audio_sample_width,\n",
    "                block_size=audio_block_size,\n",
    "                flush_size=audio_flush_size\n",
    "            )\n",
    "        )\n",
    "    if output_audio_file:\n",
    "        audio_sink = audio_helpers.WaveSink(\n",
    "            open(output_audio_file, 'wb'),\n",
    "            sample_rate=audio_sample_rate,\n",
    "            sample_width=audio_sample_width\n",
    "        )\n",
    "    else:\n",
    "        audio_sink = audio_device = (\n",
    "            audio_device or audio_helpers.SoundDeviceStream(\n",
    "                sample_rate=audio_sample_rate,\n",
    "                sample_width=audio_sample_width,\n",
    "                block_size=audio_block_size,\n",
    "                flush_size=audio_flush_size\n",
    "            )\n",
    "        )\n",
    "    # Create conversation stream with the given audio source and sink.\n",
    "    conversation_stream = audio_helpers.ConversationStream(\n",
    "        source=audio_source,\n",
    "        sink=audio_sink,\n",
    "        iter_size=audio_iter_size,\n",
    "        sample_width=audio_sample_width,\n",
    "    )\n",
    "\n",
    "    if not device_id or not device_model_id:\n",
    "        try:\n",
    "            with open(device_config) as f:\n",
    "                device = json.load(f)\n",
    "                device_id = device['id']\n",
    "                device_model_id = device['model_id']\n",
    "                logging.info(\"Using device model %s and device id %s\",\n",
    "                             device_model_id,\n",
    "                             device_id)\n",
    "        except Exception as e:\n",
    "            logging.warning('Device config not found: %s' % e)\n",
    "            logging.info('Registering device')\n",
    "            if not device_model_id:\n",
    "                logging.error('Option --device-model-id required '\n",
    "                              'when registering a device instance.')\n",
    "                sys.exit(-1)\n",
    "            if not project_id:\n",
    "                logging.error('Option --project-id required '\n",
    "                              'when registering a device instance.')\n",
    "                sys.exit(-1)\n",
    "            device_base_url = (\n",
    "                'https://%s/v1alpha2/projects/%s/devices' % (api_endpoint,\n",
    "                                                             project_id)\n",
    "            )\n",
    "            device_id = str(uuid.uuid1())\n",
    "            payload = {\n",
    "                'id': device_id,\n",
    "                'model_id': device_model_id,\n",
    "                'client_type': 'SDK_SERVICE'\n",
    "            }\n",
    "            session = google.auth.transport.requests.AuthorizedSession(\n",
    "                credentials\n",
    "            )\n",
    "            r = session.post(device_base_url, data=json.dumps(payload))\n",
    "            if r.status_code != 200:\n",
    "                logging.error('Failed to register device: %s', r.text)\n",
    "                sys.exit(-1)\n",
    "            logging.info('Device registered: %s', device_id)\n",
    "            pathlib.Path(os.path.dirname(device_config)).mkdir(exist_ok=True)\n",
    "            with open(device_config, 'w') as f:\n",
    "                json.dump(payload, f)\n",
    "\n",
    "    device_handler = device_helpers.DeviceRequestHandler(device_id)\n",
    "\n",
    "    @device_handler.command('action.devices.commands.OnOff')\n",
    "    def onoff(on):\n",
    "        if on:\n",
    "            logging.info('Turning device on')\n",
    "        else:\n",
    "            logging.info('Turning device off')\n",
    "\n",
    "    @device_handler.command('com.example.commands.BlinkLight')\n",
    "    def blink(speed, number):\n",
    "        logging.info('Blinking device %s times.' % number)\n",
    "        delay = 1\n",
    "        if speed == \"SLOWLY\":\n",
    "            delay = 2\n",
    "        elif speed == \"QUICKLY\":\n",
    "            delay = 0.5\n",
    "        for i in range(int(number)):\n",
    "            logging.info('Device is blinking.')\n",
    "            time.sleep(delay)\n",
    "\n",
    "    with SampleAssistant(lang, device_model_id, device_id,\n",
    "                         conversation_stream, display,\n",
    "                         grpc_channel, grpc_deadline,\n",
    "                         device_handler) as assistant:\n",
    "        # If file arguments are supplied:\n",
    "        # exit after the first turn of the conversation.\n",
    "        if input_audio_file or output_audio_file:\n",
    "            assistant.assist()\n",
    "            return\n",
    "\n",
    "# changed the wait for keypress to a wait for keyword using the snowboy module\n",
    "\n",
    "    # If no file arguments supplied:\n",
    "    # keep recording voice requests using the microphone\n",
    "    # and playing back assistant response using the speaker.\n",
    "    # When the once flag is set, don't wait for a trigger. Otherwise, wait.\n",
    "        wait_for_user_trigger = not once\n",
    "        while True:\n",
    "            if wait_for_user_trigger:\n",
    "                #click.pause(info='Press Enter to send a new request...\n",
    "                kws.set_callback(on_detected)\n",
    "                continue_conversation = assistant.assist()\n",
    "            # wait for user trigger if there is no follow-up turn in\n",
    "            # the conversation.\n",
    "            wait_for_user_trigger = not continue_conversation\n",
    "\n",
    "            # If we only want one conversation, break.\n",
    "            if once and (not continue_conversation):\n",
    "                break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "googa",
   "language": "python",
   "name": "googa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
